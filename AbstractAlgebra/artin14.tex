\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage{geometry, changepage, hyperref}
\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{physics, esint}
\usepackage{tikz-cd}
\usepackage{tgpagella, eulervm} 

\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=cyan}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{claim}{Claim}

\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\Ker}{\operatorname{Ker}}
\newcommand{\Coker}{\operatorname{Coker}}
\newcommand{\Ann}{\operatorname{Ann}}
\newcommand{\Spec}{\operatorname{Spec}}
\renewcommand{\longrightarrow}{\xrightarrow{\hspace*{0.7cm}}}

\newcommand{\s}{$\\ \text{ }$}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\uvec}[1]{\mathop{} \!\hat{\textbf{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\tensor}[1]{\mathsf{#1}}
\newcommand{\nll}{\operatorname{null}}
\newcommand{\range}{\operatorname{range}}
\newcommand{\cof}{\operatorname{cof}}

\title{Artin: Linear Algebra in a Ring}
\author{James Pagan}
\date{February 2024}

% --------------------------------------------- %

\begin{document}

\maketitle
\tableofcontents
\newpage

% --------------------------------------------- %

\section{Modules}

% --------------------------------------------- %

\subsection{Definition}

An \textbf{R-module} over a commutative ring $R$ is an Abelian group $M$ (with operation written additively) endowed with a mapping $\mu : R \times M \to M$ (written multiplicatively) such that the following axioms are satisfied for all $x, y \in M$ and $a, b \in R$:
\begin{enumerate}
	\item $1x = x$;
	\item $(ab)x = a(bx)$;
	\item $a(x + y) = ax + ay$;
	\item $(a + b)x = ax + bx$.
\end{enumerate}

% --------------------------------------------- %

\subsection{Examples of Modules}

\begin{itemize}
	\item If $R$ is a ring, $R[x]$ is a module.
	\item All ideals $\mathfrak{a} \subseteq R$ are $R$-modules using the same additive and multiplicative operations as $R$ --- in particular $R$ itself is an $R$-module.
	\item If $R$ is a field, $R$-modules are $R$-vector spaces. In fact, the axioms above are identical to the vector axioms, defined over commutative rings instead of fields.
	\item Abelian groups $G$ are precisely the modules over $\mathbb{Z}$.
\end{itemize}

% --------------------------------------------- %

\subsection{R-Module Homomorphisms}

A map $f: M \to N$ between two $R$-modules $M$ and $N$ is an \textbf{R-module homomorphism} (or is \textbf{R-linear}) if for all $a \in R$ and $x, y \in M$,
\begin{align*}
	f(x + y) & = f(x) + f(y) \\
	f(ax)    & = a f(x).
\end{align*}
Thus, an $R$-module homomorphism $f$ is a homomorphism of Abelian groups that commutes with the action of each $a \in R$. If $R$ is a field, an $R$-module homomorphism is a linear map. A bijective $R$-homomorphism is called an $R$-isomorphism.

The set $\Hom_{R}(M, N)$ denotes the set of all $R$-module homomorphisms from $M$ to $N$, and is a module if we define the following operations for $a \in R$ and $f, g \in \Hom_{R}(M, N)$:
  \begin{align*}
	(f + g)(x) & = f(x) + g(x) \\
	(af)(x)    & = a f(x).
\end{align*}
We denote $\Hom_{R}(M, N)$ by $\Hom(M, N)$ if the ring $R$ is unambiguous.

\begin{adjustwidth}{0.5cm}{}
	\begin{proposition}
		$\Hom_{R}(R, M) \cong M$
	\end{proposition}
	\begin{proof}
		The mapping $\phi : \Hom_{R}(R, M) \to M$ defined by $\phi(f) = f(1)$ is a homomorphism, as verified by a routine computation: for all $f, g \in \Hom_{R}(M, N)$ and $a \in R$,
		\begin{align*}
      \phi(f + g) = (f + g)(1) &= f(1) + g(1) = \phi(f) + \phi(g) \\
            \phi(af) = (af)(1) &= a f(1) = a \phi(f),
		\end{align*}
		so $\phi$ is an $R$-homomorphism. This mapping is injective, since each $f$ is uniquely determined by $f(1)$. It is also surjective; for each $m \in M$, set define a homomorphism by $h(1) = m$. Thus $\phi$ is the desired isomorphism.
	\end{proof}
\end{adjustwidth}

Homomorphisms $u : M' \to M$ and $v : N \to N''$ induce mappings $\bar{u} : \Hom(M, N) \to \Hom(M', N)$ and $\bar{v} : \Hom(M, N) \to \Hom(M, N'')$ defined for $f \in \Hom(M, N)$ as follows
\[
	\bar{u}(f) = f \circ u \qquad \text{and} \qquad \bar{v}(f) = v \circ f.
\]
I do not know why such a manipulation is noteworthy. The formulas above are quite easy to memorize if the time ever comes to invoke them.

% --------------------------------------------- %

\subsection{Submodules}

A \textbf{submodule} $M'$ of $M$ is an Abelian subgroup of $M$ closed under multiplication by elements of the commutative ring $R$. 

\begin{adjustwidth}{0.5cm}{}
  \begin{proposition}
    $\mathfrak{a}$ is an ideal of $R$ if and only if it is an $R$-submodule of $R$.
  \end{proposition}
  \begin{proof}
    The proof evolves from a fundamental observation:
    \[
      R \mathfrak{a} = \mathfrak{a} \, \iff \, \text{scalar multiplication in the $R$-module $\mathfrak{a}$ is closed}.
    \]
    The rest of the multiplicative module conditions follow from the ring axioms.
  \end{proof}
\end{adjustwidth}

The following proof outlines the construction of \textbf{quotient modules}:

\begin{adjustwidth}{0.5cm}{}
	\begin{proposition}
		The Abelian quotient group $M \, / \, M'$ is an $R$-module under the operation $a(x + M') = ax + M'$.
	\end{proposition}
	\begin{proof}
		We must perform four rather routine calculations: for all $x, y \in M$ and $a, b \in R$,
		\begin{enumerate}
      \item \textbf{Identity}: $1(x + M') = 1x + M' = x + M'$.
      \item \textbf{Compatibility}: $a(b(x + M')) = a(bx + M') = abx + M' = (ab)(x + M')$.
			\item \textbf{Left Distributivity}: $(a + b)(x + M') = (a + b)x + M' = (ax + bx) + M' = (ax + M') + (bx + M') = a(x + M') + b(x + M')$.
			\item \textbf{Right Distributivity}: $a((x + M') + (y + M')) = a((x + y) + M') = a(x + y) + M' = (ax + M') + (ay + M') = a(x + M') + a(y + M)'$.
		\end{enumerate}
		Therefore, $M / M'$ is an $R$-module. Also, this operation is naturally well-defined.
	\end{proof}
\end{adjustwidth}

$R$-module homomorphisms $f : M \to N$ induce three notable submodules: 
\begin{enumerate}
  \item \textbf{Kernel}: $\Ker f \, = \, \{ x \in M \, \mid \, f(x) = 0 \}$, a submodule of $M$.
  \item \textbf{Image}: $\Im f \, = \, \{ f(x) \, \mid \, x \in M \}$, a submodule of $N$.
  \item \textbf{Cokernel}: $\Coker f \, = \, N \, / \, \Im f$, a quotient of $N$.
\end{enumerate}
The cokernel is perhaps an unfamiliar face. Such a quotient is not possible for rings or groups; images of homomorphisms need not be ideals of $R$ nor normal subgroups of $G$. 

\begin{adjustwidth}{0.5cm}{}
  \begin{theorem}[First Isomorphism Theorem]
     $N \, / \, \Ker f \, \cong \, \Im f$.
  \end{theorem}
  \begin{proof}
    Let $K = \Ker f$, and define a mapping $g : M \, / \, N \to \Im f$ by $g(x + K) = f(x)$. We have for arbitrary $x, y \in N$ and $a \in R$ that
    \begin{align*}
      g(x + y + K) \, = \, f(x + y) \, &= \, f(x) + f(y) \, = \, g(x + K) + g(y + K). \\
            g(ax + K) \, = \, f(ax) \, &= \, a f(x) \, = \, a g(x + K).
    \end{align*}
    Hence $g$ is a homomorphism. For injectivity, suppose that $g(x + K) = g(y + K)$ --- that is, $f(x) = f(y)$. Then
    \[
      f(y - x) \, = \, f(y) - f(x) \, = \, 1,
    \]
    so $y - x \in K$. Thus $x + K = y + K$. Surjectivity is quite clear. We conclude that $g$ is the desired isomorphism.
  \end{proof}
\end{adjustwidth}

Let $f : M \to N$ be an $R$-module homomorphism. Here are two special cases of the prior theorem:
\begin{enumerate}
  \item If $f$ is a monomorphism, them $M \, \cong \, \Im f$.
  \item If $f$ is an epimorphism, then $M \, / \, \Ker f \, \cong \, N$.
\end{enumerate}

For a submodule $N' \subseteq \Im f$, I call $M' = \{ x \in M \, \mid \, f(a) \in N' \}$ the \textbf{contraction module}.

\begin{adjustwidth}{0.5cm}{}
  \begin{theorem}[Correspondence Theorem]
    Submodules of $G$ which contain $\Ker f$ correspond one-to-one with submodules of $\Im f$.
  \end{theorem}
  \begin{proof}
    For each submodule $N' \subseteq \Im f$ consider the contraction module $M' \, = \, \{ x \, \mid \, f(x) \in N' \}$. Since this is an Abelian subgroup, we need only check for multiplicative closure: for all $x \in M'$ and $a \in R$, we have
    \[
      f(ax) = a f(x) \in N' \implies ax \in N'.
    \]
    Hence $M'$ is a submodule. It is clear that $\Ker f \subseteq M'$, so the First Isomorphism Theorem yields that
    \[
      N' \, / \, \Ker f \, \cong \, M'.
    \]
    Thus this construction is injective. It is surjective, since for each $\Ker \subseteq N' \subseteq N$, the subgroup $N'$ is contracted by $f(N')$. The correspondence is now established.
  \end{proof}
\end{adjustwidth}

% -------------------------------------------

\section{Free Modules}

% --------------------------------------------- %

\subsection{R-Matrices}

The \textbf{free and finitely-generated R-modules} are the $R$-vectors with entries in $R$ and operations defined as follows:
\[
  \begin{bmatrix} r_{1} \\ \vdots \\ r_{n} \end{bmatrix} \, + \, \begin{bmatrix} s_{1} \\ \vdots \\ s_{n} \end{bmatrix} \, = \, \begin{bmatrix} r_{1} + s_{1} \\ \vdots \\ r_{n} + s_{n} \end{bmatrix} \qquad \text{and} \qquad s \, \begin{bmatrix} r_{1} \\ \vdots \\ r_{n} \end{bmatrix} \, = \, \begin{bmatrix} s r_{1} \\ \vdots \\ s r_{n} \end{bmatrix}.
\]
Analogously to fields, we can define \textbf{R-matrices} --- matrices with components in $R$ --- as $R$-module homomorphisms from $R^{n}$ to $R^{m}$. Addition and multiplication of $R$-matrices is defined as expected. The set of all $R$-module homomorphisms forms the \textbf{general linear group}:
\[
  GL_{n}(R) \, = \, \{ \text{$n$-by-$n$ invertible $R$-matrices} \}.
\]
The \textbf{determinant} of an $R$-module is computed in precisely the same way, and satisfies a similar property: if $\mat{T}$ and $\mat{S}$ are $R$-matrices capable of multiplication,
\[
  \det(\mat{TS}) \, = \, \det(\mat{T}) \det(\mat{S})
\]
There is also the \textbf{cofactor matrix}: the matrix $\cof(\mat{T})$ such that $\mat{T} \cof(\mat{T}) = \cof(\mat{T}) \mat{T} = \det(\mat{T}) \mat{I}$.

\newpage

\begin{adjustwidth}{0.5cm}{}
  \begin{lemma}
    Let $\mat{T}$ be a square $R$-matrix. Then the following holds:
    \begin{enumerate}
      \item $\mat{T}$ is invertible if and only if $\det(\mat{T})$ is a unit.
      \item $\mat{T}$ is invertible if and only if $\mat{T}$ has a one-sided inverse.
      \item If $\mat{T}$ is invertible, then $\mat{T}$ is square.
    \end{enumerate}
  \end{lemma}
  \begin{proof}
    Suppose that $\det(\mat{T})$ is a unit. Then $(\det(\mat{T})^{-1})\cof(\mat{T})$ suffices as an inverse of $\mat{T}$ by the properties of cofactor matrices; the converse holds as well. If $\mat{T}$ has a one-sided inverse $\mat{S}$, then without loss of generality,
    \[
      \det(\mat{T}) \det(\mat{S}) \, = \, \det(\mat{TS}) \, = \, \det(\mat{I}) \, = \, 1,
    \]
    so $\det(\mat{T})$ is a unit; hence $\mat{T}$ is invertible. Now, suppose that $\mat{T}$ is invertible; if $\mat{T}$ is not square, we can extend it and its inverse $\mat{S}$ by adding rows (or columns) of zeroes. This yields the following equation without loss of generality:
    \[
      \left[
      \begin{array}{c|c}
        \\ \mat{T} \quad  & \quad 0 \\ \\
      \end{array}
      \right] \left[
      \begin{array}{c}
        \quad \mat{S} \quad \\ \\ \hline \\ 0
      \end{array}
      \right] \quad = \quad \mat{I}.
    \]
    This is a contradiction, since the left-hand side has determinant $0$ and the right-hand side has determinant $1$.
  \end{proof}
\end{adjustwidth}

When $R$ has few units, invertibility is strong condition. For instance, a $\mathbb{Z}$-matrix is invertible if and only if its determinant is $\pm 1$. Thus $GL_{n}(\mathbb{Z}) \subset GL_{n}(\mathbb{R})$; of all integer matrices that are invertible as $\mathbb{R}$-matrices, few are invertible as $\mathbb{Z}$-matrices.

% --------------------------------------------- %

\subsection{Free Modules}

A \textbf{free R-module} is an $R$-module that has a \textbf{basis}: a spanning set of independent elements. Compare this with the definition delineated in AbstractAlgebra/atiyah2.tex. We mean that $x_{1}, \ldots, x_{n} \in M$ are \textbf{independent} if
\[
  a_{1} x_{1} + \cdots + a_{n} x_{n} = 0 \, \implies \, a_{1} = \cdots = a_{n} = 0.
\]
The \textbf{rank} of a free $R$-module is the length of its basis. Clearly $\operatorname{rank} M = n$ if and only if $M \, \cong \, R^{n}$. Anyways, a \textbf{finitely-generated R-module} is an $R$-module hat contains $x_{1}, \ldots, x_{n} \in M$ such that
\[
  M \, = \, R x_{1} + \cdots + R c_{n} \, = \, \{ a_{1}x_{1} + \cdots + a_{n}x_{n} \, \mid \, a_{1}, \ldots, a_{n} \in R \}.
\]
An independent set of generators is called a \textbf{basis}. As with vector spaces, $x_{1}, \ldots, x_{n} \in M$ is a basis of $M$ if and only if all elements of $M$ are a unique linear combination of $x_{1}, \ldots, x_{n}$. The \textbf{canonical basis} of $R^{n}$ consists of $\vec{e}_{1}, \ldots, \vec{e}_{n}$.
\newpage
\textit{Most modules have no basis}. A free $\mathbb{Z}$-module is \textbf{free Abelian group}. Finite Abelian groups are never free, since each element has finite additive order:
\[
  o(x_{1}) x_{1} + \cdots o(x_{n}) x_{n} \, = \, 0 + \cdots + 0 \, = \, 0
\]
Let $\vec{B} = (x_{1}, \ldots, x_{n})$ be a basis of a free $R$-module $M$. Then $\vec{B}$ induces a homomorphism $R^{n} \stackrel{\vec{B}}{\longrightarrow} M$ defined by
\[
  \vec{B}X \, = \, \begin{bmatrix} x_{1} & \cdots & x_{n} \end{bmatrix} \begin{bmatrix} a_{1} \\ \vdots \\ a_{n} \end{bmatrix} \, = \, a_{1}x_{1} + \cdots + a_{n}x_{n}.
\]
This homomorphism is injective if elements of $\vec{B}$ is independent, surjective $\vec{B}$ generates $M$, and bijective if $\vec{B}$ constitute a basis of $R^{n}$. Hence $M$ has a basis of length $n$ if and only if $M \, \cong \, R^{n}$. I \textit{really} don't like this notation, but I guess I have to live with it.

% --------------------------------------------- %

\subsection{Matrices in Free Modules}

Let $\mat{B}$ be the basis of a free $R$-module $M$. The \textbf{coordinate vector} $X$ of an element $\vec{v} \in M$ is the unique column vector such that $\vec{v} = \mat{B} X$. If $\mat{B}'$ is a change of basis, the relevant formula is $\mat{B}' = \mat{B}P$. We assert the following proposition without proof:

\begin{adjustwidth}{0.5cm}{}
  \begin{proposition}
    The following two properties of bases hold:
    \begin{enumerate}
      \item A matrix $\mat{T}$ of a change-of-basis in a free module is an invertible $R$-matrix.
      \item All bases of a free $R$-module have the same cardinality. \s
    \end{enumerate}
  \end{proposition}
\end{adjustwidth}

Let $M$ and $N$ be free $R$-modules with bases $\mat{B} = (x_{1}, \ldots, x_{n})$ and $\mat{C} = (y_{1}, \ldots, y_{m})$ respectively. All $R$-module homomorphisms $f : M \to N$ admit the form of left-multiplication by an $m$-by-$n$ $R$-matrix $\mat{T} = (t_{ij})$, with components given by
\[
  f(y_{j}) \, = \, \sum\limits_{i = 1}^{n} x_{i} t_{ij}
\]
If $X$ is the coordinate vector of $\vec{v} \in M$ --- namely, if $\vec{v} = \mat{B}X$ --- then $Y = \mat{T}X$ is the coordinate vector of its image.
\[
  \begin{tikzcd}
    R^n \arrow[d, "\mat{B}"] \arrow[r, "\mat{T}"] & R^m \arrow[d, "\mat{C}"] \\
    M \arrow[r, "f"]                              & N                       
  \end{tikzcd}
  \qquad \iff \qquad 
  \begin{tikzcd}
    X \arrow[d, dashed] \arrow[r, dashed] & Y \arrow[d, dashed] \\
    \vec{v} \arrow[r, dashed]             & f(\vec{v})         
  \end{tikzcd}
\]
Let the bases $\mat{B}$ and $\mat{C}$ change by invertible $R$-matrices $\mat{S}$ and $\mat{R}$. Then if $\mat{T}$ is the $R$-matrix of $f : M \to N$, the new formula for $\mat{T}$ is the same for vector spaces: $\mat{T} \, ' \, = \, \mat{R}^{-1} \mat{T} \mat{S}$.

% --------------------------------------------- %

% DISCLAIMER: Since this section is not relevant to Homework 5, I will return to its discussion later down the line.

% --------------------------------------------- %

% \section{Identities}
%
% This section is an informal discussion. It investigates the question: why do properties of $F$-matrices hold when their components are substituted with elements from a ring?

% --------------------------------------------- %

\section{Diagonalizing Integer Matrices}

The critical question is as follows: given an $m$-by-$n$ $\mathbb{Z}$-matrix $\mat{T}$ and a vector $\vec{B} \in \mathbb{Z}^{m}$, when does there exist $\vec{A} \in \mathbb{Z}^{n}$ such that
\[
  \mat{T}\vec{A} \, = \, \vec{B}?
\]
The most important of these questions is when $\mat{T} \vec{A} = \vec{0}$. In a field, one often performs row reduction --- but deprived of multiplicative inverses, most row reductions are not allowed. Rather, we allow both row \textit{and} column reduction, that being any of the following:
\begin{enumerate}
  \item Add an integer multiple of a row to a row or a column to a column.
  \item Interchange two rows or two columns.
  \item Multiply a row or column by $-1$.
\end{enumerate}

Any such operation can be performed by multiplying $\mat{T}$ by an \textbf{elementary integer matrix}, which is always invertible. The final result of a sequence of operations has the form
\[
  \mat{T} \, ' \, = \, \mat{Q}^{-1} \mat{T} \mat{P},
\]
where $\mat{Q}^{-1}$ and $\mat{T}$ are invertible $\mathbb{Z}$-matrices of the appropriate sizes. $\mat{Q}^{-1}$ documents row operations, while $\mat{P}$ dictates column operations: those in $\mat{P}$ are multiplied in the same order as performed, while those in $\mat{Q}$ are in \textit{reverse} order.

\begin{adjustwidth}{0.5cm}{}
  \begin{theorem}
    Let $\mat{T}$ be an $m$-by-$n$ integer matrix. Then there exist invertible matrices $P$ and $Q$ such that $Q^{-1}TP$ is diagonal --- say,
    \[
      \begin{bmatrix} \begin{bmatrix} d_{1} & \, & \, \\ \, & \ddots & \, \\ \, & \, & d_{k} \end{bmatrix}  & \, \\ \, & 0 \end{bmatrix},
    \]
    where $d_{i}$ are positive and $d_{1} \mid \cdots \mid d_{k}$.
  \end{theorem}
  \newpage
  \begin{proof}
    We present a rather unusual proof: an algorithmic one. The strategy is to reduce $\mat{A}$ to a matrix of the form
    \begin{equation}
      \begin{bmatrix} d_{1} & \cdots & 0 \\ \vdots & \begin{bmatrix} \, & \, & \, \\ \, & \mat{M} & \, \\ \, \end{bmatrix} \\ 0 \end{bmatrix},
    \end{equation}
    where $\mat{M}$ extends down to the bottom of the matrix (hard to draw!).
    \begin{enumerate}
      \item \textbf{Step 1}: Permute the rows and columns such that the $a_{ij}$ with the smallest absolute value to the upper left corner. If necessary, multiply by $-1$ such that this element is positive.
      \item \textbf{Step 2}: If the first column contains a nonzero element $a_{i1}$, divide it by $a_{11}$: we have
      \[
        a_{i1} \, = \, a_{11}q + r,
      \]
      where $a_{11} > r \ge 0$. If $r > 0$, perform the relevant row operation such that $a_{i1}$ becomes $r$ and go to Step 1. If $r = 0$, then repeat Step 2. If there are no nonzero elements, proceed to Step 3.
      \item \textbf{Step 3}: If the first row contains a nonzero element $a_{1j}$, divide it by $a_{11}$: we have
      \[
        a_{1j} \, = \, a_{11}q + r,
      \]
      where $a_{11} > r \ge 0$. If $r > 0$, perform the relevant column operation such that $a_{i1}$ becomes $r$ and go to Step 1. If $r = 0$, then repeat Step 3. If there are no nonzero elements, proceed to Step 4.
      \item \textbf{Step 4}: We attain a matrix of the form in Equation (1). Suppose that some element of $\mat{M}$ is not divisible by $d_{1}$. Add this column into the first column and return to Step 1; this will yield an $a_{11}$ of smaller absolute value. If no such elements exist, proceed to Step 5.
      \item \textbf{Step 5}: An easy induction on argument on $\max \{ m, n \}$ now implies that $\mat{T}$ can be factored into the required form.
    \end{enumerate}
    Observe that we exclusively return to earlier steps when $\abs{a_{11}}$ decreases. This can happen only finitely many times, so no step will ever repeat infinitely often. Then this algorithm indeed yields us a matrix of the desired form.
  \end{proof}
\end{adjustwidth}

This proof isn't exactly rigorous, but it's still quite cool. I think you could formalize this via the classification of finitely-generated modules over PIDs. In any case, it ensures the existence of invertible integer matrices $\mat{Q}$ and $\mat{P}$ such that for all $\mat{T} \in \mathcal{L}(\mathbb{Z}^{n}, \mathbb{Z}^{n})$, we have
\[
  \mat{T} \, ' \, = \, \mat{Q}^{-1}\mat{T}\mat{P},
\]
where $\mat{T} \, '$ has the form of Theorem 3.

\newpage

We are ready to solve the equation $\mat{T} \vec{A} \, = \, \vec{B}$.

\begin{adjustwidth}{0.5cm}{}
  \begin{proposition}
    Let $\mat{T} \, ' \, = \, \mat{Q}^{-1}\mat{T}\mat{P}$ as before. Then the following hold:
    \begin{enumerate}
      \item The integer solutions to the equation $\mat{T} \, ' \vec{A}' \, = \, \vec{0}$ are the vectors $\mat{A}$ whose first $k$ components are $0$.
      \item The integer solutions to the equation $\mat{T} \vec{A} = \vec{0}$ are those of the form $\mat{A} = \mat{P} \vec{A}'$, where $\mat{T} \, ' \vec{A}' = \vec{0}$.
      \item The image $W'$ of multiplication by $\mat{T} \,'$ is the integer combinations of the coordinate vectors $d_{1} \vec{e}_{1}, \ldots, d_{k} \vec{e}_{k}$.
      \item The image $W$ of multiplication by $\mat{T}$ is the integer combinations of the coordinate vectors $\mat{Q} (d_{1} \vec{e}_{1}), \ldots, \mat{Q} (d_{k} \vec{e}_{k})$.
    \end{enumerate}
  \end{proposition}
  \begin{proof}
    (1) is implied since $\mat{T}'$ is diagonal: the equation $\mat{T} \, ' \vec{A}'$ for $\vec{A} = (a_{1}, \ldots, a_{n})$ reads
    \[
      d_{1}a_{1} \, = \, 0, \quad d_{2}a_{2} \, = \, 0, \quad \ldots \quad d_{k}a_{k} \, = \, 0.
    \]
    Hence there exists a solution if and only if $a_{1} = \cdots = a_{k} = 0$. Both (2) and (4) can be viewed as change of bases --- in which case, the matrix $\mat{P}$ carries the kernel of $\mat{T}$ to the kernel of $\mat{T}'$, while $\mat{Q}$ carries the image of $\mat{T} \, '$ to the image of $\mat{T}$.

    As for (3), it is quite easy to deduce that $\mat{T} \, '$ maps all $\vec{A} = (a_{1}, \ldots, a_{n})$ to the vector $(d_{1}a_{1}, \ldots, d_{k}a_{k}, 0, \ldots, 0)$. The vectors $d_{1} \vec{e}_{1}, \ldots, d_{k} \vec{e}_{k}$ clearly span this space.
  \end{proof}
\end{adjustwidth}

Isn't this solution so simple and elegant? This section discussed computation and theory together, like some cosmic marble cake. But I digress: the basis of vectors described in (4) is not unique. I'm not sure if the matrix $\mat{A}'$ is unique, but it seems like it should be?

% --------------------------------------------- %

\subsection{Subgroups of Free Abelian Groups}

Theorem 4 on diagonalization of $\mathbb{Z}$-matrices describes homomorphisms of Abelian groups.

\begin{adjustwidth}{0.5cm}{}
  \begin{corollary}
    Let $\phi : G \to H$ be a homomorphism of free Abelian groups. Then there exist bases of $G$ and $H$ such that the matrix of $\phi$ is diagonal. \s
  \end{corollary}
\end{adjustwidth}

This section would ideally discuss $R$-submodules of free $R$-modules, where $R$ is a principal ideal domain. Unfortunately, integer matrices are no help here; the proof of Theorem 4 relied upon the Euclidean algorithm. Thus we instead focus on $\mathbb{Z}$-modules.

\begin{adjustwidth}{0.5cm}{}
  \begin{theorem}
    Let $G$ be a free Abelian group of rank $n$ and let $H \subseteq G$ be a subgroup. Then $H$ is a free Abelian group of rank $n$ or smaller.
  \end{theorem}
  \begin{proof}
    By Theorem 5, $H$ is finitely generated. Thus let $\vec{G} = (g_{1}, \ldots, g_{m})$ and $\vec{H} = (h_{1}, \ldots, h_{n})$ be bases of $G$ and $H$. Thus if we set $h_{j} = \sum_{i} g_{i} a_{ij}$, the elements $a_{ij}$ form the components of the $\mat{T}$ matrix associated with the inclusion mapping $i : G \to H$:
    \[
      \begin{tikzcd}
        \mathbb{Z}^m \arrow[r, "\mat{T}"] \arrow[d, "\vec{H}"] & \mathbb{Z}^n \arrow[d, "\vec{G}"] \\
           H \arrow[r, "i"]                                    & G                                
      \end{tikzcd}
    \]
    Since $\vec{G}$ is a basis, the right-hand arrow is bijective; since $\vec{H}$ generates $H$, the left-hand arrow is surjective.

    Diagonalize $\mat{T}$ to the form $\mat{T} \, ' \, = \, \mat{Q}^{-1} \mat{T} \mat{P}$ for invertible matrices $\mat{P}$ and $\mat{Q}$. Thus we can interpret $\mat{Q}$ as a change of basis in $\mathbb{Z}^{m}$; since our original choice of $\vec{G}$ and $\vec{H}$ were arbitrary, we can substitute them into our commutative diagram. We find an isomorphism $\mathbb{Z}^{m} \, \cong \, H$, so $H$ is free.
  \end{proof}
\end{adjustwidth}

This proof actually misses a few edge cases --- but frankly I just don't give a shit right now. I'll return to this over the weekend.

% --------------------------------------------- %

\section{Presentation Matrices}

Left multiplication by an $m$-by-$n$ $R$-matrix $\mat{T}$ induces an $R$-module homomorphism
\[
  R^{n} \stackrel{\mat{T}}{\longrightarrow} R^{m}. 
\]
The image of $\mat{T}$ consists of all linear combinations of the columns of $\mat{T}$ with coefficients in the ring; we may denote this ring by $\mat{T} R^{n}$. We say that the quotient module $M \, = \, R^{m} \, / \, \mat{T} R^{n}$ is \textbf{presented} by $\mat{T}$. 

More generally, any isomorphism $\sigma : R^{m} \, / \, \mat{T} R^{n} \to M$ is a \textbf{presentation} of $M$, where the $R$-matrix $\mat{T}$ is a \textbf{presentation matrix} of $M$. For instance, $C_{5}$ is presented by the integer matrix $[5]$ since $C_{5} \, \cong \, \mathbb{Z} \, / \, 5 \mathbb{Z}$.

We can utilize the canonical epimorphism $\pi : R^{m} \to R^{m} \, / \, \mat{T} R^{n}$ to interpret $M$ as follows:

\begin{adjustwidth}{0.5cm}{}
  \begin{proposition}
    Let $\pi : R^{m} \to R^{m} \, / \, \mat{T} R_{n}$ be the canonical epimorphism. Then 
    \begin{enumerate}
      \item $M$ is generated by $\vec{B} = (\vec{e}_{1}, \ldots, \vec{e}_{m})$, the images of the standard basis of $R^{m}$.
      \item If $\vec{Y} = (y_{1}, \ldots, y_{m}) \in R^{m}$, the element $\mat{B}\vec{Y} = y_{1} \vec{e}_{1} + \cdots y_{m} \vec{e}_{m}$ is zero if and only if $Y$ is a linear combination of the columns of $\vec{T}$ --- which is to say, if and only if $\vec{Y}$ lies in the image of $\mat{T}$.
    \end{enumerate}
  \end{proposition}
  \begin{proof}
    (1) is a trivial consequence of the surjectivity of $\pi$. As per (2), we have that
    \begin{align*}
      \text{$\vec{BY} = \vec{0}$} & \iff \text{$\vec{BY} \in \mat{T} R^{n}$} \\ 
                                  & \iff \text{$\vec{Y}$ lies in the image of $\mat{T}$} \\
                                  & \iff \text{$\vec{Y}$ is a linear combination of the columns of $\mat{T}$}.
    \end{align*}
    This completes the proof.
  \end{proof}
\end{adjustwidth}

If a module $M$ is generated by a set $\vec{B} = (x_{1}, \ldots, x_{m})$, we call an element $\vec{Y} = (y_{1}, \ldots, y_{m}) \in R^{m}$ such that $\vec{BY} = y_{1}x_{1} + \cdots + y_{n}x_{n} = 0$ a \textbf{relation vector} of the generators. The equation $y_{1}x_{1} + \cdots + y_{m}x_{m} = 0$ is called a \textbf{relation}. A set $S$ of relations is \textbf{complete} if each relation is a linear combination of relations in $S$.

\textbf{Example 1.} Consider an Abelian group $G$ generated by $a, b, c$ with the complete set of relations
\begin{align*}
  3a \, + \, 2b \, + \, c &= \, 0 \\
  8a \, + \, 4b \, + \, 2c \, &= \, 0 \\
  7a \, + \, 6b \, + \, 2c \, &= \, 0 \\
  9a \, + \, 6b \, + \, c \, &= \, 0.
\end{align*}
This group is presented by the following matrix:
\[
  \mat{T} = \begin{bmatrix} 3 & 8 & 7 & 9 \\ 2 & 4 & 6 & 6 \\ 1 & 2 & 2 & 1 \end{bmatrix}
\]
Its columns are the coefficients of the relations described above: $(x_{1}, x_{2}, x_{3}) \mat{T} \, = \, (0, 0, 0)$.

\subsection{Translating between Presentations and Modules}

We now delineate a method to find a presentation for an $R$-module $M$. We make two assumptions, both of which are easily satisfied if $R$ is Noetherian:
\begin{enumerate}
  \item $M$ is finitely generated --- say, by $\vec{B} = (x_{1}, \ldots, x_{m})$.
  \item The module $W$ of relations of $\vec{B}$ is finitely generated.
\end{enumerate}

The generators $\vec{B}$ entail an epimorphism $R^{m} \stackrel{\vec{B}}{\longrightarrow} M$ that maps a column vector $\vec{Y} = (y_{1}, \ldots, y_{m})$ to the element $y_{1}x_{1} + \cdots + y_{m}x_{m}$. The kernel of this homomorphism is $W$: the module of relations of $\vec{B}$. By the First Isomorphism Theorem, we have
\[
  M \, \cong \, R^{m} \, / \, W.
\]
We turn our attention to $W$. Since $W$ is finitely generated, there exists a set of generators $\vec{C} = (w_{1}, \ldots, w_{n})$ from which we obtain an epimorphism $R^{n} \stackrel{\vec{C}}{\longrightarrow} W$. The generators $\vec{w}_{i} \in R^{m}$ may be arranged into a matrix as follows:
\[
  \mat{T} \quad = \quad \begin{bmatrix} \vdots & \vdots & \, & \vdots \\ \vec{w}_{1} & \vec{w}_{2} & \cdots & \vec{w}_{n} \\ \vdots & \vdots & \, & \vdots \end{bmatrix}.
\]
This $n$-by-$m$ $R$-matrix $\mat{T}$ is a composition of $R^{n} \to W$ with the embedding $W \subset R^{m}$. By construction, its image is $W$ --- which we may denote as $\mat{T}R^{m}$. Thus we have
\[
  M \, \cong \, R^{m} \, / \, W \, = \, R^{m} \, / \, \mat{T}R^{n}.
\]
$\mat{T}$ is a presentation of $M$. Observe that since $\mat{T}$ depends on $\vec{B}$ and $\vec{C}$, there are many potential presentations of $M$. In fact:

\begin{adjustwidth}{0.5cm}{}
  \begin{proposition}
    Let $\mat{T}$ be an $m$-by-$n$ presentation matrix of an $R$-module $M$. Then the following matrices $\mat{T} \, '$ also present $M$:
    \begin{enumerate}
      \item $\mat{T}\,' \, = \, \mat{Q}^{-1}\mat{T}$, where $\mat{Q} \in GL_{m}(R)$.
      \item $\mat{T}\,' \, = \, \mat{T} \mat{P}$, where $\mat{P} \in GL_{n}(R)$.
      \item $\mat{T}\,'$ obtained by deleting a column of zeroes.
      \item If the $j$-th column of $\mat{T}$ is $\vec{e}_{i}$, the matrix $\mat{T}\,'$ obtained by deleting row $i$ and column $j$.
    \end{enumerate}
  \end{proposition}
  \begin{proof}
    The proofs originate from the following observations:
    \begin{enumerate}
      \item The change of $\mat{T}$ to $\mat{Q}^{-1}\mat{T}$ corresponds to a change of basis in $R^{m}$ --- in other words, an isomorphism.
      \item The change of $\mat{T}$ to $\mat{T}\mat{P}$ corresponds to a change of basis in $R^{n}$ --- in other words, an isomorphism.
      \newpage
      \item A column of zeroes corresponds to the trivial relation, which can be omitted.
      \item A column of $\mat{T}$ equal to $\vec{e}_{i}$ corresponds to the relation $\vec{B}(\vec{e}_{i}) = 0$. The zero element is useless as a generator --- so we can simply cleave it away from the generating set and the relations. Doing so changes $R^{n}$ and $R^{m}$ to $R^{n - 1}$ and $R^{m - 1}$, and changes the matrix $\mat{T}$ by deleting the $i$-th row and $j$-th column.
    \end{enumerate}
    This concludes the proof.
  \end{proof}
\end{adjustwidth}

This provides a clean method for determining an $R$-module from its presentation. For the Abelian group in our example, it reduces to
\begin{align*}
  \mat{T} \, & = \, \begin{bmatrix} 3 & 8 & 7 & 9 \\ 2 & 4 & 6 & 6 \\ 1 & 2 & 2 & 1 \end{bmatrix} \implies \begin{bmatrix} 1 & 2 & 2 & 1 \\ 0 & 0 & 2 & 4 \\ 0 & 2 & 1 & 6 \end{bmatrix} \implies \begin{bmatrix} 0 & 2 & 4 \\ 2 & 1 & 6 \end{bmatrix} \implies \begin{bmatrix} -4 & 0 & -8 \\ 2 & 1 & 6 \end{bmatrix} \\
             & \implies \begin{bmatrix} -4 & -8 \end{bmatrix} \implies \begin{bmatrix} 4 & 0 \end{bmatrix} \implies \begin{bmatrix} 4 \end{bmatrix}.
\end{align*}
Thus $\mat{T}$ presents the Abelian group $\mathbb{Z}_{4}$.

% --------------------------------------------- %

\section{Noetherian Rings and Modules}

An $R$-module $M$ is \textbf{Noetherian} if all ascending chains of submodules
\[
  M_{1} \, \subseteq \, M_{2} \, \subseteq \, M_{3} \, \subseteq \, \cdots
\]
are stationary: there exists $i$ such that $M_{i} = M_{i + 1} = \cdots$. A ring $R$ is \textbf{Noetherian} if it is a Noetherian module over itself; namely, if all ascending chains of ideals are stationary.

\begin{adjustwidth}{0.5cm}{}
  \begin{proposition}
    An $R$-module $M$ is Noetherian if and only if all submodules of $M$ are finitely generated.
  \end{proposition}
  \begin{proof}
    Suppose that all submodules of $M$ are finitely generated, and let 
    \[
      M_{1} \, \subseteq \, M_{2} \, \subseteq \, M_{3} \, \subseteq \, \cdots
    \]
    be a chain of submodules in $M$. Then $N = \bigcup_{i = 1}^{\infty} M_{i}$ is a submodule of $M$, hence is finitely generated by some $x_{1}, \ldots, x_{n}$. Define $n_{i}$ such that $x_{i} \in M_{n_{i}}$; letting $n = \max\{ n_{1}, \ldots, n_{i} \}$, it is clear that all $x_{i} \in M_{n}$, so $M_{n} = N$. Hence the chain is stationary.

    If there exists a submodule of $M$ which is infinitely generated by $x_{1}, x_{2}, \ldots$ (possibly with uncountably many generators), then
    \[
      (x_{1}) \, \subseteq \, (x_{1}, x_{2}) \, \subseteq \, (x_{1}, x_{2}, x_{3}) \, \subseteq \, \cdots
    \]
    is an ascending chain in $M$ which is not stationary; hence $M$ is not Noetherian.
  \end{proof}
\end{adjustwidth}

Likewise, $R$ is Noetherian if and only if all ideals of $R$ are finitely-generated. This proposition highlights the importance of Noetherian modules; the Noetherian condition is just the right finiteness condition to make a lot of theorems work.

\begin{adjustwidth}{0.5cm}{}
  \begin{corollary}
    Every proper ideal $(a)$ is contained within some maximal ideal $\mathfrak{m}$.
  \end{corollary}
  \begin{proof}
    By Krull's Theorem, the quotient ring $R \, / \, (a)$ contains a maximal ideal $\mathfrak{m}'$. By the Correspondence Theorem, the contraction of $\mathfrak{m}'$ is a maximal ideal $\mathfrak{m} \subset R$ which contains $(a)$.
  \end{proof}
\end{adjustwidth}

We assert without proof that direct sums and quotients of Noetherian modules are Noetherian; the proof involves exact sequences, which we defer to notes on Atiyah-MacDonald. In any case, the Noetherian condition relates to finite generation:

\begin{adjustwidth}{0.5cm}{}
  \begin{theorem}
    Let $R$ be a Noetherian ring. Then all finitely-generated $R$-modules are Noetherian.
  \end{theorem}
  \begin{proof}
    Let $R$ be Noetherian. Via our claims without proof, we have that
    \begin{align*}
      \text{$M$ is finitely-generated} \, &\iff \, \text{$M \, \cong \, R^{n} \, / \, \mathfrak{a}$} \\
                                          &\implies \, \text{$M$ is Noetherian}.
    \end{align*}
    This completes the proof.
  \end{proof}
  \begin{corollary}
    Let $M$ be a finitely generated module over a Noetherian ring. Then all submodules of $M$ are finitely generated. \s
  \end{corollary}
\end{adjustwidth}

FUCK!

% --------------------------------------------- %

\end{document}g
