\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage{geometry, changepage}
\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{physics}
\usepackage{hyperref}

\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=cyan}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\uvec}[1]{\mathop{} \!\hat{\mathbf{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\tensor}[1]{\mathsf{#1}}

\renewcommand{\div}{\nabla \cdot}
\renewcommand{\curl}{\nabla \cross}
\renewcommand{\grad}{\nabla}
\renewcommand{\laplacian}{\nabla^{2}}

\title{MATH-UA 140: Assignment 2}
\author{James Pagan, September 2023}
\date{Professor Raqu√©pas}

% --------------------------------------------- %

\begin{document}

\maketitle
\tableofcontents

% --------------------------------------------- %

\section{Problem 1}

\textbf{Part (a)}: Across all $x_{1}, x_{2} \in \mathbb{R}$, the expression
\[
	\begin{bmatrix} 3 & 5 \\ -2 & 0 \\ 8 & 9 \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} = x_{1} \begin{bmatrix} 3 \\ -2 \\ 8 \end{bmatrix} + x_{2} \begin{bmatrix} 5 \\ 0 \\ 9 \end{bmatrix}
\]
is the span of $(3, -2, 8)$ and $(5, 0, 9)$. However, these two vectors and $(2, -3, 8)$ are linearly independent, as
\[
	\begin{vmatrix} 3 & 5 & 2 \\ -2 & 0 & -3 \\ 8 & 9 & 8 \end{vmatrix} = 0 + (-120) + (-36) - (0) - (-80) - (-81) = 5 \ne 0.
\]
If $x_{1}$ and $x_{2}$ existed such that $x_{1} (3, -2, 8) + x_{2} (5, 0, 9) = (2, -3, 8)$, then
\[
	x_{1} \begin{bmatrix} 3 \\ -2 \\ 8 \end{bmatrix} + x_{2} \begin{bmatrix} 5 \\ 0 \\ 9 \end{bmatrix} - \begin{bmatrix} 2 \\ -3 \\ 8 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix},
\]
which contradicts the fact that the three vectors are linearly independent. We conclude that no such $x_{1}$ and $x_{2}$ exist.

% --------------------------------------------- %

\textbf{Part (b)}: Adding four times the first row to the third row yields
\[
	\begin{bmatrix} 1 & -2 & 1 \\ 0 & 2 & -8 \\ 0 & -3 & 13 \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \\ x_{3} \end{bmatrix} = \begin{bmatrix} 0 \\ 8 \\ -9 \end{bmatrix}.
\]
Adding two-thirds of the second row to the third row yields
\[
	\begin{bmatrix} 1 & -2 & 1 \\ 0 & 2 & -8 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \\ x_{3} \end{bmatrix} = \begin{bmatrix} 0 \\ 8 \\ 3 \end{bmatrix}.
\]
We thus have three equations:
\begin{align*}
	& x_{3} = 3 \\
	& 2x_{2} - 8x_{3} = 8 \implies 2x_{2} - 24 = 8 \implies x_{2} = 16 \\
	& x_{1} - 2x_{2} + x_{3} = 0 \implies x_{1} - 32 + 3 = 0 \implies x_{1} = 29
\end{align*}
The answer is thus $(x_{1}, x_{2}, x_{3}) = (29, 16, 3)$.

% --------------------------------------------- %

\section{Problem 2}

\textbf{Part (a)} For all $k \in \mathbb{R}$, the vector $(1, 0, 0)$ is a solution to $A \vec{x} = \vec{b}$: 
\[
	\begin{bmatrix} 1 & 1 & 1 \\ 2 & 4 & 4 \\ 3 & 7 & k \end{bmatrix} \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 + 0 + 0 \\ 2 + 0 + 0 \\ 3 + 0 + 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}.
\]
There is \textbf{no} $\mathbf{k}$\textbf{-value} such that $(1, 0, 0)$ is not a solution to $A \vec{x} = \vec{b}$.

\textbf{Part (b)}: Observe that $\vec{b}$ lies in the span of the columns of $A$ for all $k \in \mathbb{R}$, as the first column of $A$ is $\vec{b}$ itself. Therefore, there exist infinitely many solutions to $A \vec{x} = \vec{b}$ when the columns of $A$ are linearly dependent, which occurs if $\det(A)$ is zero: namely, if  
\[
	0 = \begin{vmatrix} 1 & 1 & 1 \\ 2 & 4 & 4 \\ 3 & 7 & k \end{vmatrix} = 4k + 12 + 14 - 12 - 2k - 28 = 2k - 14.
\]
and $\mathbf{k = 7}$.

\textbf{Part (c)}: There exists exactly one solution $\vec{b}'$ to $A \vec{x} = \vec{b}'$ whenever the columns of $A$ are linearly independent --- which occurs if $\det(A)$ is nonzero. Our work in Part (b) establishes that this holds for all reals $\mathbf{k \ne 7}$

\textbf{Part (d)}: Observe that
\begin{align*}
	\begin{bmatrix} 1 & 0 & 0 \\ -2 & 1 & 0 \\ 1 & -2 & 1 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\ 2 & 4 & 4 \\ 3 & 7 & 10 \end{bmatrix} &= \left( \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -2 & 1 \end{bmatrix} \begin{bmatrix} 1 &  0 & 0 \\ -2 & 1 & 0 \\ -3 & 0 & 1 \end{bmatrix} \right) \begin{bmatrix} 1 & 1 & 1 \\ 2 & 4 & 4 \\ 3 & 7 & 10 \end{bmatrix} \\
	&= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -2 & 1 \end{bmatrix} \left( \begin{bmatrix} 1 & 0 & 0 \\ -2 & 1 & 0 \\ -3 & 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\ 2 & 4 & 4 \\ 3 & 7 & 10 \end{bmatrix} \right) \\
	&= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -2 & 1 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\ 0 & 2 & 2 \\ 0 & 4 & 7 \end{bmatrix} \\
	&= \begin{bmatrix} 1 & 1 & 1 \\ 0 & 2 & 2 \\ 0 & 0 & 3 \end{bmatrix}.
\end{align*}

The matricies we seek are thus
\[
	E = \begin{bmatrix} 1 & 0 & 0 \\ -2 & 1 & 0 \\ 1 & -2 & 1 \end{bmatrix} \qquad \text{and} \qquad U = \begin{bmatrix} 1 & 1 & 1 \\ 0 & 2 & 2 \\ 0 & 0 & 3 \end{bmatrix}.
\]

% --------------------------------------------- %

\section{Problem 3}

The matrix is invertible if and only if the determinant is nonzero; namely, if 
\[
	0 \ne \begin{vmatrix} 2 & c & c \\ c & 5 & c \\ 8 & c & c \end{vmatrix} = 10c + 8c^{2} + c^{3} - 40c - c^{3} - 2c^{2} = 6c^{2} - 30c,
\]
or all $\mathbf{c \in \mathbb{R} \setminus \{ 0, 5 \}}$.

% --------------------------------------------- %

\section{Problem 4}

Applying the eliminations $E_{21}$, $E_{31}$, and $E_{41}$ yields
\[
	\begin{bmatrix} a & a & a & a \\ 0 & b-a & b - a & b - a \\ 0 & b - a & c - a & c - a \\ 0 & b - a & c - a & d - a \end{bmatrix}.
\]
Further applying the eliminations $E_{32}$ and $E_{42}$ yields
\[
	\begin{bmatrix} a & a & a & a \\ 0 & b - a & b - a & b - a \\ 0 & 0 & c - b & c - b \\ 0 & 0 & c - b & d - b \end{bmatrix}.
\]
Finally, further applying $E_{43}$ yields an upper trianular matrix:
\[
	\begin{bmatrix} a & a & a & a \\ 0 & b-a & b-a & b-a \\ 0 & 0 & c-b & c-b \\ 0 & 0 & 0 & d-c \end{bmatrix}
\]
% --------------------------------------------- %

\section{Problem 5}

\textbf{Part (a)} The result is \textbf{true}. If $A$ is an $n$-by-$n$ matrix with a column of zeroes, then multiplying $A$ with any other $n$-by-$n$ matrix will clearly produce a matrix with the same column of zeroes. As the identity matrix has no column of zeroes, we deduce there is no $A^{-1}$ such that $A A^{-1} = I$.

\textbf{Part (b)} The result is \textbf{true}. For every row of an $n$-by-$n$ matrix $A$ to add to zero, we must have that
\begin{align*}
	a_{1, 1} + a_{1,2} + \cdots & + a_{1, n - 1} = - a_{1, n}, \\
	& \vdots \\
	a_{n, 1} + a_{n,2} + \cdots & + a_{n, n - 1} = - a_{n, n}.
\end{align*}
Then adding every column vector of $A$ \textit{except} the final column produces the negative of the final column. Thus, the columns of A are not linearly independent, and $\det A = 0$; then $A$ is singular.

\textbf{Part (c)} The result is \textbf{true}. For every column of an $n$-by-$n$ matrix $A$ to add to zero, we must have that
\begin{align*}
	a_{1, 1} + a_{2,1} + \cdots & + a_{n - 1, 1} = - a_{n, 1}, \\
	& \vdots \\
	a_{1, n} + a_{2,n} + \cdots & a_{n - 1, n} = - a_{n, n}.
\end{align*}
Then adding every row vector of $A$ \textit{except} the final row produces the negative of the final row. Thus, the row of A are not linearly independent, and $\det A = 0$; then $A$ is singular.

\textbf{Part (d)} The result is \textbf{false}. Consider the $2$-by-$2$ matrix 
\[
	\begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}.
\]
It has $1$'s down its main diagonal, yet has determinant $1 - 1 = 0$; it is therefore not intervtible, so the answer is \textbf{false}.

\textbf{Part (e)} The result is \textbf{true}. If $A$ is invertible, then the inverse of $A^{-1}$ is $A$; furthermore,
\begin{align*}
	A^{2} A^{-2} = A A A^{-1} A^{-1} = A (I) A^{-1} = A A^{-1} = I, \\
	A^{-2} A^{2} = A^{-1} A^{-1} A A = A^{-1} (I) A = A^{-1} A = I,
\end{align*}
so $A^{2}$ has inverse $A^{-2}$. 

% --------------------------------------------- %

\end{document}
