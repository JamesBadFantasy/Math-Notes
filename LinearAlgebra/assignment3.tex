\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage{geometry, changepage}
\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{physics}
\usepackage{hyperref}

\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=cyan}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\uvec}[1]{\mathop{} \!\hat{\mathbf{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\tensor}[1]{\mathsf{#1}}

\renewcommand{\div}{\nabla \cdot}
\renewcommand{\curl}{\nabla \cross}
\renewcommand{\grad}{\nabla}
\renewcommand{\laplacian}{\nabla^{2}}

\title{MATH-UA 140: Assignment 3}
\author{James Pagan, October 2023}
\date{Professor Raqu√©pas}

% --------------------------------------------- %

\begin{document}

\maketitle
\tableofcontents

% --------------------------------------------- %

\section{Problem 1}

\textbf{Part (a)}: Observe that as $B \vec{x} = \vec{0}$
\[
	\vec{x} = EB \vec{x} = E (B \vec{x}) = E \vec{0} = \vec{0};
\]
thus, $\boxed{\vec{x} \in \operatorname{null} U \setminus \{ \vec{0} \}}$.

\textbf{Part (b)}: Observe that if $U \vec{y} = 0$, then $EB \vec{y} = \vec{0}$. Thus, $E (B \vec{y}) = \vec{0}$, so $B \vec{y}$ is in the null spacae of $E$. As $E$ is invertible, its null space consists exclusibely of the zero vector --- therefore, $B \vec{y} = \vec{0}$, and $\boxed{\vec{y} \in \operatorname{null} U \setminus \{ \vec{0} \}}$.


\textbf{Part (c)}: The above result demonstrates that $U \vec{x} = \vec{0}$ if and only if $B \vec{x} = 0$. We may view this matrix-vector multiplication as a linear combination of the columns of $U$ or $B$ --- doing so yields that a linear combination of the columns of $B$ gives $\vec{0}$ if and only if $\boxed{\text{the same combination of the columns of $U$ gives $\vec{0}$}}$.

% --------------------------------------------- %

\section{Problem 2}

\textbf{Part (a)}: $\boxed{\text{yes}}$; for example
\[
	\begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} 3 & 1 & 4 \\ 0 & 0 & 1 \\ 0 & 5 & 2 \end{bmatrix}
\]
after performing the multiplication has a different column space.

\textbf{Part (b)}: $\boxed{\text{no}}$, performing elimination cannot change the null space of the matrix.

\textbf{Part (c)}: $\boxed{\text{no}}$, performing elimination cannot change the row space of the matrix.

\textbf{Part (d)}: $\boxed{\text{no}}$, performing elimination will not change the column rank.

\textbf{Part (e)}: $\boxed{\text{no}}$, performing elimination will not change the row rank.

\textbf{Part (f)}: $\boxed{\text{no}}$, performing elimination will not change the nullity.

% --------------------------------------------- %

\section{Problem 3}

\textbf{Part (a)}: The column vectors
\[
	\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 2 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}
\]
are clearly independent, and the first and second may be combined to generate the third via 
\[
	-2 \pi \begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix} + \pi \begin{bmatrix} 2 \\ 1 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ \pi \\ 0 \\ 0 \end{bmatrix},
\]
so the $\boxed{\text{first, second, and fourth}}$ are the fewest possible columns that span the column space of $A$.

\textbf{Part (b)}: The dimension is $\boxed{3}$, as the first, second, and fourth rows are independent (by the placement of their zeroes), and the third row may be achieved as a trivial linear combination of the others (namely, with scalars equal to zero).

\textbf{Part (c)}: Such a matrix is trivially the permutation matrix
\[
	\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \end{bmatrix}.
\]

The Fundamental Theorem of Linear Maps guarantees that because the column space has dimension three, the null space has dimension one. Further observe that 
\[
	\begin{bmatrix} 1 & 2 & 0 & 1 \\ 0 & 1 & \pi & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 2\pi \\ -\pi \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix},
\]
so the $\boxed{\text{span of $(2\pi, -\pi, 1, 0)$}}$ is the null space of $A$ by necessity of dimension.

% --------------------------------------------- %

\section{Problem 4}

We have that
\[
	(A^{-1}B^{\top})^{\top} A^{\top} = (B^{\top})^{\top} (A^{1})^{\top} A^{\top} = B (A A^{-1})^{\top} = B I^{\top} = BI = \boxed{B}.
\]

% --------------------------------------------- %

\section{Problem 5}

\textbf{Part (a)}: Clearly, the element of $\mathbb{R}^{n}$ is $\lambda \vec{x} + ;u \vec{y}$.

\textbf{Part (b)}: The dot product we seek is
\[
	\vec{x} \cdot \vec{y} = \boxed{\sum_{i = 1}^{n} x_{i}y_{i}}
\]
\textbf{Part (c)}: The matrix-vector product is
\[
	\begin{bmatrix} x_{1} & \cdots & x_{n} \end{bmatrix} \begin{bmatrix} y^{1} \\ \vdots \\ y_{n} \end{bmatrix} = \boxed{\sum_{i = 1}^{n} x_{i}y_{i}}.
\]
\textbf{Part (d)}: The matrix product is
\[
	\begin{bmatrix} y_{1} \\ \vdots \\ y_{3} \end{bmatrix} \begin{bmatrix} x_{1} & \cdots & x_{3} \end{bmatrix} = \begin{bmatrix} y_{1}x_{1} & \cdots & y_{1}x_{3} \\ \vdots & & \vdots \\ y_{3}x_{1} & \cdots & y_{3}x_{3} \end{bmatrix}
\]
\textbf{Part (e)}: The column rank is $\boxed{1}$, as every column of the matrix is a scalar multiple of the other via the $x$-values.

\end{document}
