\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage{geometry, changepage}
\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{physics, tikz}
\usepackage{hyperref}

\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=cyan}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\uvec}[1]{\mathop{} \!\hat{\mathbf{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\tensor}[1]{\mathsf{#1}}

\renewcommand{\div}{\nabla \cdot}
\renewcommand{\curl}{\nabla \cross}
\renewcommand{\grad}{\nabla}
\renewcommand{\laplacian}{\nabla^{2}}

\title{MATH-UA 140: Assignment 7}
\author{James Pagan, November 2023}
\date{Professor Raqu√©pas}

% --------------------------------------------- %

\begin{document}

\maketitle
\tableofcontents

% --------------------------------------------- %

\section{Problem 1}

We have that
\begin{align*}
	Q \vec{x} \cdot Q \vec{y} &= (Q \vec{x})^{\top} Q \vec{y} \\
	&= (\vec{x}^{\top}Q^{\top})Q \vec{y} \\
	&= \vec{x}^{\top} (Q^{\top}Q) \vec{y} \\
	&= \vec{x}^{\top} (I) \vec{y} \\
	&= \vec{x}^{\top} \vec{y} \\
	&= \vec{x} \cdot \vec{y}
\end{align*}

% --------------------------------------------- %

\section{Problem 2}

Let the three vectors yielded by Gram-Schmidt \textbf{\textit{without normalization}} be $\vec{g}_{1}$, $\vec{g}_{2}$, and $\vec{g}_{3}$, and denote the three given vectors by $\vec{v}_{1}$, $\vec{v}_{2}$, and $\vec{v}_{3}$ respectively.

We have that
\begin{align*}
	\vec{g}_{1} &= \vec{v}_{1} = \begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix}, \\
	\vec{g}_{2} &= \vec{v}_{2} - \operatorname{proj}_{\vec{v}_{1}}(\vec{v}_{2}) \\
				&= \vec{v}_{2} - \frac{\vec{v}_{1} \cdot \vec{v}_{2}}{\norm{\vec{v}_{1}}^{2}} \vec{v}_{1} \\
				&= \vec{v}_{2} + \frac{1}{5} \vec{v}_{1} \\
				&= \begin{bmatrix} -0.8 \\ 0.4 \\ 1 \end{bmatrix}, \\
	\vec{g}_{3} &= \vec{v}_{3} - \operatorname{proj}_{\vec{v}_{1}}(\vec{v}_{3}) - \operatorname{proj}_{\vec{v}_{2}}(\vec{v}_{3}) \\
				&= \vec{v}_{3} - \frac{\vec{v}_{1} \cdot \vec{v}_{3}}{\norm{\vec{v}_{1}}^{2}}\vec{v}_{1} - \frac{\vec{v}_{2}\cdot \vec{v}_{3}}{\norm{\vec{v}_{2}}^{2}} \vec{v}_{2} \\
				&= \vec{v}_{3} - \frac{2}{5} \vec{v}_{1} + \vec{v}_{2} \\
				&= \begin{bmatrix} 2/3 \\ -1/3 \\ 2/3 \end{bmatrix},
\end{align*}
We must now normalize these vectors, which yields the following three:
\[
	\boxed{ \begin{bmatrix} \tfrac{\sqrt{5}}{5} \\  \\ \tfrac{2\sqrt{5}}{5} \\ \\ 0 \end{bmatrix}, \begin{bmatrix} -\frac{4 \sqrt{5}}{15} \\\\ \frac{2\sqrt{5}}{15} \\\\ \frac{\sqrt{5}}{3} \end{bmatrix}, \begin{bmatrix} \frac{2}{3} \\\\ -\frac{1}{3} \\\\ \frac{2}{3} \end{bmatrix}}.
\]

% --------------------------------------------- %

\section{Problem 3}

\textbf{Part (a)}: We have that
\[
	A^{\top} A = \begin{bmatrix} 1/2 & 0 & 0 \\ 1 & 1 & 0 \end{bmatrix} \begin{bmatrix} 1/2 & 1 \\ 0 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1/4 & 1/2 \\ 1/2 & 2 \end{bmatrix}. 
\]
It is now trivial to verify via that this matrix has determininat $\tfrac{1}{4}$, so its inverse is
\[
	(A^{\top}A)^{1} \begin{bmatrix} 8 & -2 \\ -2 & 1 \end{bmatrix}.
\]
Therefore, the projection matrix is
\begin{align*}
	A(A^{\top}A)^{-1}A^{\top} &= \begin{bmatrix} 1/2 & 1 \\ 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 8 & -2 \\ -2 & 1 \end{bmatrix} \begin{bmatrix} 1/2 & 0 & 0 \\ 1 & 1 & 0 \end{bmatrix} \\
							  &= \begin{bmatrix} 2 & 0 \\ -2 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 1/2 & 0 & 0 \\ 1 & 1 & 0 \end{bmatrix} \\
							  &= \boxed{ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}.}
\end{align*}

\textbf{Part (b)}: Performing the Graham-Schmidt process, we find that the first vector is
\[
	\vec{v}_{1} = \begin{bmatrix} 1/2 \\ 0 \\ 0 \end{bmatrix}
\]
and the second vector is 
\[
	\vec{v_{2}} = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} - \operatorname{proj}_{\vec{v}_{1}} \left( \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} \right) = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} - \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}.
\]
The normalization of these vectors is $\vec{u} = (1, 0, 0)$ and $\vec{v} = (0, 1, 0)$, so
\[
	\vec{u}\vec{u}^{\top} + \vec{v} \vec{v}^{\top} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} \begin{bmatrix} 0 & 1 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} + \begin{bmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix} = \boxed{ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}}.
\]
% --------------------------------------------- %

\section{Problem 4}

\textbf{Part (a)}: For four collinear points $(a, b)$, $(c, d)$, $(e, f)$, $(g, h)$ on the line $y = \alpha x + \beta$, we have that
\[
	\begin{bmatrix} a & 1 \\ c & 1 \\ e & 1 \\ g & 1 \end{bmatrix} \begin{bmatrix} \alpha \\ \beta \end{bmatrix} = \begin{bmatrix} b \\ d \\ f \\ h \end{bmatrix}.
\]
We can therefore compute the least-squares regression line by projecting
\[
	\begin{bmatrix} 3 \\ 1 \\ -2 \\ -5 \end{bmatrix} \qquad \text{onto} \qquad \begin{bmatrix} -2 & 1 \\ -1 & 1 \\ 1 & 1 \\ 2 & 1 \end{bmatrix} = M.
\]
We can do this via computing the projection matrix $M (M^{\top}M)^{-1}M^{\top}$:
\[
	M^{\top}M = \begin{bmatrix} -2 & -1 & 1 & 2 \\ 1 & 1 & 1 & 1 \end{bmatrix} \begin{bmatrix} -2 & 1 \\ -1 & 1 \\ 1 & 1 \\ 2 & 1 \end{bmatrix} = \begin{bmatrix} 10 & 0 \\ 0 & 4 \end{bmatrix}.
\]
The inverse of this matrix is clearly 
\[
	(M^{\top}M)^{-1} = \begin{bmatrix} 1/10 & 0 \\ 0 & 1/4 \end{bmatrix}.
\]
Hence,
\begin{align*}
	(M^{\top}M)^{-1}M^{\top} &= \begin{bmatrix} 1/10 & 0 \\ 0 & 1/4 \end{bmatrix} \begin{bmatrix} -2 & -1 & 1 & 2 \\ 1 & 1 & 1 & 1 \end{bmatrix} \\
							 &= \begin{bmatrix} -1/5 & -1/10 & 1/10 & 1/5 \\ 1/4 & 1/4 & 1/4 & 1/4 \end{bmatrix}	
\end{align*}
Therefore, we seek
\[
	\begin{bmatrix} -1/5 & -1/10 & 1/10 & 1/5 \\ 1/4 & 1/4 & 1/4 & 1/4 \end{bmatrix} \begin{bmatrix} 3 \\ 1 \\ -2 \\ -5 \end{bmatrix} = \begin{bmatrix} -19/10 \\ -3/4 \end{bmatrix}
\]
Therefore, our line has equation $\boxed{\frac{-19}{10}x - \frac{3}{4}}$.

\textbf{Part (b)}: The graph is given by (pardon the oversized diagram)

\begin{tikzpicture}
	\draw[help lines, color=gray!30, dashed] (-10,-10) grid (10, 10);
	\draw[<->,thick] (-10,0)--(10,0) node[right]{$x$};
	\draw[<->,thick] (0,-10)--(0,10) node[above]{$y$};
	\draw[<->,thick, blue] (-5.658,10)--(4.868,-10) node[right]{$y = -\frac{19}{10}x - \frac{3}{4}$};
	\foreach \Point in {(-2,3), (-1,1), (1,-2), (2,-5)}{
   		\node at \Point {\textbullet};
	}
\end{tikzpicture}

% --------------------------------------------- %

\section{Problem 5}
We have that $P^{2} = P$ and $P^{\top} = P$, so 
\begin{align*}
	B^{\top}B &= (I - 2P)^{\top} (I - 2P) \\
			  &= (I^{\top} - 2P^{\top})(I - 2P) \\
			  &= (I - 2P)(I - 2P) \\
			  &= I - 4P + 4P^{2} \\
			  &= I - 4P + 4P \\
			  &= I. \\
\end{align*}
Thus, $B$ is an orthogonal matrix.

\end{document}
