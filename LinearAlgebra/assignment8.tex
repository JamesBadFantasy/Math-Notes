\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage{geometry, changepage}
\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{physics}
\usepackage{hyperref}

\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=cyan}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\uvec}[1]{\mathop{} \!\hat{\mathbf{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\tensor}[1]{\mathsf{#1}}

\renewcommand{\div}{\nabla \cdot}
\renewcommand{\curl}{\nabla \cross}
\renewcommand{\grad}{\nabla}
\renewcommand{\laplacian}{\nabla^{2}}

\title{MATH-UA 140: Assignment 8}
\author{James Pagan, December 2023}
\date{Professor Raqu√©pas}

% --------------------------------------------- %

\begin{document}

\maketitle
\tableofcontents
\newpage

% --------------------------------------------- %

\section{Problem 1}

\textbf{Part (a)}: Suppose for contradiction that two lists of reals $\alpha_{1}, \ldots, \alpha_{n}$ and $\beta_{1}, \ldots, \beta_{n}$ --- not all equal --- satisfy
\[
	\alpha_{1}\vec{v}_{1} + \cdots + \alpha_{n}\vec{v}_{n} = \beta_{1}\vec{v}_{1} + \cdots + \beta_{n}\vec{v}_{n}.
\]
We may simplfy this equation to get that
\[
	(\alpha_{1} - \beta_{1})\vec{v}_{1} + \cdots + (\alpha_{n} - \beta_{n})\vec{v}_{n} = \vec{0}.
\]
As $\vec{v}_{1}, \ldots, \vec{v}_{n}$ are independent, the only solution to this equation is when
\begin{align*}
	\alpha_{1} - \beta_{1} &= 0 \\
						   & \vdots \\
	\alpha_{n} - \beta_{n} &= 0.
\end{align*}
Then $\alpha_{j} = \beta_{j}$ for all $j \in \{ 1, \ldots, n \}$, which contradicts our definition of $\alpha_{1}, \ldots, \alpha_{n}$ and $\beta_{1}, \ldots, \beta_{n}$ as distinct lists. We conclude that the choice of $\alpha_{1}, \ldots, \alpha_{n}$ is unique.

\textbf{Part (b)}: Such a matrix $J$ maps the canonical basis of $\mathbb{R}^{n}$ to $\vec{v}_{1}, \ldots, \vec{v}_{n}$ --- so each $\vec{v}_{1}, \ldots, \vec{v}_{n}$ lie in the range of $J$. This list spans $\mathbb{R}^{n}$, so $\operatorname{range} J = \mathbb{R}^{n}$. Then as $\dim \operatorname{range} J = n$, the operator $J$ is surjective. Hence,
\[
	n = \dim \mathbb{R}^{n} = \dim \operatorname{range} J + \dim \operatorname{null} J = n + \dim \operatorname{null} J
\]
so $\dim \operatorname{null} J = 0$. Then $J$ is injective, and is thus invertible.

\textbf{Part (c)}: Let $v_{ij}$ be the $i$-th entry of the $j$-th vector --- so $\vec{v}_{j} = (v_{1j}, v_{2j}, \ldots, v_{nj})$. Then the following matrix $J$ maps $\vec{e}_{j}$ to $\vec{v}_{j}$ for all $j \in \{ 1, \ldots, n \}$:
\[
	J = \begin{bmatrix} v_{11} & \cdots & v_{1n} \\ \vdots & \ddots & \vdots \\ v_{n1} & \cdots & v_{nn} \end{bmatrix}.
\]
By expanding the $j$-th column in the matrix resulting from the product
\[
	\begin{bmatrix} v_{11} & \cdots & v_{1n} \\ \vdots & \ddots & \vdots \\ v_{n1} & \cdots & v_{nn} \end{bmatrix} \begin{bmatrix} \gamma_{11} & \cdots & \gamma_{1n} \\ \vdots & \ddots & \vdots \\ \gamma_{n1} & \cdots & \gamma_{nn} \end{bmatrix},
\]
we find that the sum $\gamma_{1j} v_{r1} + \gamma_{2j} v_{r2} + \cdots + \gamma_{nj} v_{rn}$ for row $r \in \{ 1, \ldots, n \}$ is one when $r = j$ and zero when $r \ne j$. Then for all $j \in \{ 1, \ldots, n \}$,
\begin{align*}
	\gamma_{1j} \vec{v}_{1} + \gamma_{2j} \vec{v}_{2} + \cdots + \gamma_{nj} \vec{v}_{n} &= \gamma_{1j} \begin{bmatrix} v_{11} \\ \vdots \\ v_{n1} \end{bmatrix} + \cdots + \gamma_{nj} \begin{bmatrix} v_{1n} \\ \vdots \\ v_{nn} \end{bmatrix}\\
	&= \begin{bmatrix} \gamma_{1j} v_{11} + \cdots + \gamma_{nj} v_{1n} \\ \vdots \\ \gamma_{1j} v_{n1} + \cdots + \gamma_{n} v_{nn} \end{bmatrix} \\
	&= \vec{e}_{j}, 
\end{align*}
as desired.

% --------------------------------------------- %

\section{Problem 2}

\textbf{Part (a)}: We have that
\[
	\det(A - \lambda I) = \begin{vmatrix} a - \lambda & b \\ c & d - \lambda \end{vmatrix} = (a - \lambda)(d - \lambda) - bc = \boxed{\lambda^{2} - (a + d) \lambda + ad - bc}.
\]

\textbf{Part (b)}: The number of real roots depends on the sign of the discriminant $\beta^{2} - 4\alpha\gamma$:
\begin{itemize}
	\item \textbf{No Real Zeros}: If $\beta^{2} - 4\alpha \gamma < 0$
	\item \textbf{One Real Zero}: If $\beta^{2} - 4\alpha \gamma = 0$.
	\item \textbf{Two Real Zeroes}: If $\beta^{2} - 4\alpha \gamma > 0$.
\end{itemize}

\textbf{Part (c)}: The discriminant of our polynomial in Part (a) is
\[
	(- a - d)^{2} - 4(ad - bc)(1) = a^{2} + 2ad + d^{2} - 4ad + 4bc = (a - d)^{2} + 4bc.
\]
The number of \textbf{real} eigenvalues thus depends on the sign of this quantity:
\begin{itemize}
	\item \textbf{No Real Eigenvalues}: If $(a - d)^{2} + 4bc < 0$.
	\item \textbf{One Real Eigenvalue}: If $(a - d)^{2} + 4bc = 0$.
	\item \textbf{Two Real Eigenvalues}: If $(a - d)^{2} + 4bc > 0$
\end{itemize}

\textbf{Part (d)}: The matrix
\[
	\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
\]
has no real eigenvalues; it is a rotation matrix by $90^{\circ}$, so it displaces every nonzero vector off its span. The matrix
\[
	\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
\]
has one real eigenvalue --- that being $1$, held by every nonzero vector --- and the matrix
\[
	\begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix}
\]
has two real eigenvalues --- them being $1$ and $2$, held by the vectors $(0, 1)$ and $(1, 0)$.

% --------------------------------------------- %

\section{Problem 3}

\textbf{Part (a)}: We have that for all eigenvalues $\lambda$,
\[
	0 = \begin{vmatrix} 1 - \lambda & \tfrac{1}{4} \\ \tfrac{1}{4} & 1 - \lambda \end{vmatrix} = (1 - \lambda)^{2} - \tfrac{1}{16}
\]
so $\boxed{\lambda = \tfrac{3}{4}, \tfrac{5}{4}}$. Two eigenvectors with these eigenvalues are 
\[
	\boxed{\begin{bmatrix} -1 \\ 1 \end{bmatrix} \quad \text{and} \quad \begin{bmatrix} 1 \\ 1 \end{bmatrix}}
\]
respectively, as verified by a trivial computation.

\textbf{Part (b)}: We have that for all eigenvalues $\lambda$,
\[
	0 = \begin{vmatrix} 1 - \lambda & 1 \\ 0 & 2 - \lambda \end{vmatrix} = (1 - \lambda)(2 - \lambda) - 0 = (1 - \lambda)(2 - \lambda)
\]
so $\boxed{\lambda = 1, 2}$. Two eigenvectors with these eigenvalues are
\[
	\boxed{ \begin{bmatrix} 1 \\ 0 \end{bmatrix} \quad \text{and} \quad \begin{bmatrix} 1 \\ 1 \end{bmatrix}}
\]
respectively, as verified by a trivial computation.

\textbf{Part (c)}: We have that for all eigenvalues $\lambda$, 
\[
	0 = \begin{vmatrix} 2 - \lambda & 2 \\ 2 & 2 - \lambda \end{vmatrix} = (2 - \lambda)^{2} - 4 = \lambda^{2} - 4\lambda = \lambda(\lambda - 4),
\]
so $\boxed{\lambda = 0, 4}$. Two eigenvectors with these eigenvalues are
\[
	\boxed{\begin{bmatrix} 1 \\ -1 \end{bmatrix} \quad \text{and} \quad \begin{bmatrix} 1 \\ 1 \end{bmatrix}}
\]
respectively, as verified by a trivial calculation.

\section{Problem 4}

\textbf{Part (a)}: By the basic properties of limits,
\[
	\lim\limits_{\lambda \to \infty} -\lambda^{3} + \beta_{2} \lambda^{2} + \beta_{1} \lambda + \beta_{0} = -\infty \qquad \text{and} \qquad \lim\limits_{\lambda \to -\infty} -\lambda^{3} + \beta_{2} \lambda^{2} + \beta_{1} \lambda + \beta_{0} = \infty.
\]
Therefore, we should \textit{expect} the Intermediate Value Theorem to guarantee that this polynomial always achieves at least one real zero --- or for $3$-by-$3$ matricies to have exactly one real eigenvalue.

\textbf{Part (b)}: Consider the $3$-by-$3$ identity matrix: for all its eigenvalues,
\[
	0 = \begin{vmatrix} 1 - \lambda & 0 & 0 \\ 0 & 1 - \lambda & 0 \\ 0 & 0 & 1 - \lambda \end{vmatrix} = (1 - \lambda)^{3},
\]
so $\lambda = 1$ is the only eigenvalue; every nonzero vector is an eigenvector with this eigenvalue. Thus, one such matrix is
\[
	\boxed{ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}}.
\]

% --------------------------------------------- %

\end{document}
